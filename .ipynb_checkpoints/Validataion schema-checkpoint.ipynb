{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b387c6",
   "metadata": {},
   "source": [
    "### Validation schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437116e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra\n",
    "import numpy as np\n",
    "\n",
    "#working with data in table structers\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# working with files\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# to off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# validation schema \n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3e721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using ray engine for parallel calculation(for oprimization)\n",
    "# %env MODIN_ENGINE=ray\n",
    "# import modin.pandas as mpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b039bbbc",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed819f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data path to sys.path \n",
    "train_test_data_path = \"C:\\\\Repository\\\\DS-Intership-data\\\\train_test_data\\\\\"\n",
    "sys.path.append(train_test_data_path)\n",
    "\n",
    "# initiate dict for data\n",
    "to_read_train_test_data = {}\n",
    "\n",
    "# fill to_read\n",
    "for dir_name, _, files in os.walk(train_test_data_path):\n",
    "    for file in files:\n",
    "        to_read_train_test_data[file] = dir_name + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252c8586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_submission.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\train_test_data\\\\sample_submission.csv',\n",
       " 'test_data.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\train_test_data\\\\test_data.csv',\n",
       " 'train_data.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\train_test_data\\\\train_data.csv'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to_read\n",
    "to_read_train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf1415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.36 s\n",
      "Wall time: 5.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = {}\n",
    "# read data\n",
    "for file, path in to_read_train_test_data.items():\n",
    "    data[file.split('.')[0]] = pd.read_csv(os.path.join(os.path.dirname(path), file), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edab7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fix in data preprocessing (bag of memory usage reduce)\n",
    "data['train_data'] = data['train_data'][data['train_data'].profit != float('inf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d88eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                NaN\n",
       "1                NaN\n",
       "2                NaN\n",
       "3                NaN\n",
       "4           20995.00\n",
       "              ...   \n",
       "10902713   149575.00\n",
       "10902714   148799.00\n",
       "10902715   149242.00\n",
       "10902716   152328.00\n",
       "10902717   148377.00\n",
       "Name: ID, Length: 10902718, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to select predicts we only need for submision\n",
    "data['train_data'].merge(data['test_data'], on=['item_id', 'shop_id'], how='left').ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181d836",
   "metadata": {},
   "source": [
    "### Validation schema creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1b6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The following indexes will be used:\n",
    "      date_block_num\n",
    "      shop_id\n",
    "      item_category_id\n",
    "      item_id\n",
    "      item_cnt_month\n",
    "      \n",
    "    Concept:\n",
    "    Apply expanding window validation\n",
    "    Monthly predictions\n",
    "\"\"\"\n",
    "\n",
    "class Validation:\n",
    "    def __init__(self, data, metrics=['rmse'], n_splits=10):\n",
    "        self.data = data\n",
    "        self.metrics = metrics\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "        assert set([\n",
    "                    'date_block_num',\n",
    "                    'shop_id',\n",
    "                    'item_category_id',\n",
    "                    'item_id',\n",
    "                    'item_cnt_month'\n",
    "                   ]).issubset(data.columns),\\\n",
    "                \"Invalid data\"\n",
    "        \n",
    "    def calculate_metrics(self, y_pred, y_true):\n",
    "        rmse = mse(y_true, y_pred, squared=True)\n",
    "        return rmse\n",
    "\n",
    "    def evaluate(self, model=DecisionTreeRegressor(max_depth=5)):\n",
    "        eval_report = {}\n",
    "        tscv = TimeSeriesSplit(n_splits=self.n_splits)\n",
    "        X = data['train_data'].drop(columns='item_cnt_month')\n",
    "        y = data['train_data'].item_cnt_month\n",
    "        step = 0\n",
    "        for train, test in tscv.split(y):\n",
    "            step += 1\n",
    "\n",
    "            # Split data step\n",
    "            y_tr, y_ts = y.iloc[train].values, y.iloc[test].values\n",
    "            X_tr, X_ts = X.iloc[train].values, X.iloc[test].values\n",
    "\n",
    "            # Train step\n",
    "            rng = np.random.RandomState(42)\n",
    "            model = model\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "            # Evaluation step\n",
    "            y_tr_pr = model.predict(X_tr)\n",
    "            y_ts_pr = model.predict(X_ts)\n",
    "            \n",
    "            # Metrics\n",
    "            eval_report[\"step\"+str(step)] = {\n",
    "                \"train\\test limits\" : f\"TRAIN: from {train.min()} to  {train.max()}  (size: {train.max() - train.min()} ) \" +\n",
    "                                      f\"TEST: from {test.min()} to  {test.max()}  (size: {test.max() - test.min()} )\",\n",
    "                \"error\" : [self.calculate_metrics(y_tr_pr, y_tr), self.calculate_metrics(y_ts_pr, y_ts)],\n",
    "                \"feature_importance\" : model.feature_importances_,\n",
    "                \"__________________\" : \"_________________________________________________________________________________\"\n",
    "            }\n",
    "        return eval_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05fb74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 12s\n",
      "Wall time: 1min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'step1': {'train\\test limits': 'TRAIN: from 0 to  991157  (size: 991157 ) TEST: from 991158 to  1982313  (size: 991155 )',\n",
       "  'error': [2.7755703599570585, 2.877392044566422],\n",
       "  'feature_importance': array([4.51336510e-02, 1.43183142e-01, 3.38873785e-01, 6.66430652e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 5.01583643e-02, 2.26861119e-02,\n",
       "         1.41614549e-07, 3.33613351e-01, 5.96871475e-02, 0.00000000e+00]),\n",
       "  '__________________': '_________________________________________________________________________________'},\n",
       " 'step2': {'train\\test limits': 'TRAIN: from 0 to  1982313  (size: 1982313 ) TEST: from 1982314 to  2973469  (size: 991155 )',\n",
       "  'error': [2.7439262660143213, 5.836563234664521],\n",
       "  'feature_importance': array([0.01768405, 0.13007455, 0.22700182, 0.49410234, 0.        ,\n",
       "         0.        , 0.        , 0.06125952, 0.02491591, 0.01919449,\n",
       "         0.02576732, 0.        ]),\n",
       "  '__________________': '_________________________________________________________________________________'},\n",
       " 'step3': {'train\\test limits': 'TRAIN: from 0 to  2973469  (size: 2973469 ) TEST: from 2973470 to  3964625  (size: 991155 )',\n",
       "  'error': [3.7078342104157147, 6.047617527270213],\n",
       "  'feature_importance': array([0.        , 0.17562378, 0.422056  , 0.30450781, 0.        ,\n",
       "         0.        , 0.05703992, 0.        , 0.0136883 , 0.0147406 ,\n",
       "         0.01234359, 0.        ]),\n",
       "  '__________________': '_________________________________________________________________________________'},\n",
       " 'step4': {'train\\test limits': 'TRAIN: from 0 to  3964625  (size: 3964625 ) TEST: from 3964626 to  4955781  (size: 991155 )',\n",
       "  'error': [4.283002197862794, 5.6761508640276],\n",
       "  'feature_importance': array([0.        , 0.11475464, 0.06421863, 0.7579957 , 0.        ,\n",
       "         0.        , 0.00170577, 0.02519021, 0.01954686, 0.01658819,\n",
       "         0.        , 0.        ]),\n",
       "  '__________________': '_________________________________________________________________________________'},\n",
       " 'step5': {'train\\test limits': 'TRAIN: from 0 to  4955781  (size: 4955781 ) TEST: from 4955782 to  5946937  (size: 991155 )',\n",
       "  'error': [3.340544121496047, 3.0142923809785933],\n",
       "  'feature_importance': array([0.08995688, 0.01342698, 0.82920387, 0.05177284, 0.        ,\n",
       "         0.        , 0.00360559, 0.00390616, 0.00145323, 0.00336842,\n",
       "         0.        , 0.00330604]),\n",
       "  '__________________': '_________________________________________________________________________________'},\n",
       " 'step6': {'train\\test limits': 'TRAIN: from 0 to  5946937  (size: 5946937 ) TEST: from 5946938 to  6938093  (size: 991155 )',\n",
       "  'error': [3.2823924124272263, 3.958716054080201],\n",
       "  'feature_importance': array([6.72036346e-02, 1.23659502e-02, 8.59377045e-01, 5.08109693e-02,\n",
       "         0.00000000e+00, 0.00000000e+00, 4.69682818e-04, 2.06009060e-03,\n",
       "         1.86851097e-03, 3.14263980e-03, 0.00000000e+00, 2.70147629e-03]),\n",
       "  '__________________': '_________________________________________________________________________________'},\n",
       " 'step7': {'train\\test limits': 'TRAIN: from 0 to  6938093  (size: 6938093 ) TEST: from 6938094 to  7929249  (size: 991155 )',\n",
       "  'error': [3.219333318940979, 4.875984915846234],\n",
       "  'feature_importance': array([5.29341981e-02, 1.30250050e-02, 8.73748759e-01, 4.93357047e-02,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.99009282e-04, 2.36129739e-04,\n",
       "         1.73736069e-03, 8.58383366e-03, 0.00000000e+00, 0.00000000e+00]),\n",
       "  '__________________': '_________________________________________________________________________________'},\n",
       " 'step8': {'train\\test limits': 'TRAIN: from 0 to  7929249  (size: 7929249 ) TEST: from 7929250 to  8920405  (size: 991155 )',\n",
       "  'error': [3.4091081851318723, 4.1479199938710964],\n",
       "  'feature_importance': array([0.00542024, 0.05467054, 0.87696746, 0.03974931, 0.        ,\n",
       "         0.00180119, 0.00760738, 0.00189922, 0.00158927, 0.01029538,\n",
       "         0.        , 0.        ]),\n",
       "  '__________________': '_________________________________________________________________________________'},\n",
       " 'step9': {'train\\test limits': 'TRAIN: from 0 to  8920405  (size: 8920405 ) TEST: from 8920406 to  9911561  (size: 991155 )',\n",
       "  'error': [3.4909202709785943, 4.582669682594713],\n",
       "  'feature_importance': array([1.13559860e-02, 4.84467264e-02, 8.77430869e-01, 3.56401749e-02,\n",
       "         0.00000000e+00, 3.83810908e-03, 8.71728779e-03, 0.00000000e+00,\n",
       "         2.59644758e-03, 1.04123242e-02, 1.00217973e-03, 5.59895117e-04]),\n",
       "  '__________________': '_________________________________________________________________________________'},\n",
       " 'step10': {'train\\test limits': 'TRAIN: from 0 to  9911561  (size: 9911561 ) TEST: from 9911562 to  10902717  (size: 991155 )',\n",
       "  'error': [3.5102470443836067, 4.1466834469295515],\n",
       "  'feature_importance': array([0.00895902, 0.00667193, 0.87351167, 0.04484969, 0.        ,\n",
       "         0.00435609, 0.00571476, 0.        , 0.0460025 , 0.00901086,\n",
       "         0.        , 0.00092348]),\n",
       "  '__________________': '_________________________________________________________________________________'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val = Validation(data['train_data'])\n",
    "val.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
