{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dad3468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra\n",
    "import numpy as np\n",
    "\n",
    "#working with data in table structers\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# data visualization\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# working with files\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# to off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data preprocessing\n",
    "from itertools import product\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9693c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using ray engine for parallel calculation(for oprimization)\n",
    "# %env MODIN_ENGINE=ray\n",
    "# import modin.pandas as mpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df0f65",
   "metadata": {},
   "source": [
    "###### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a435bd0",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319cd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data path to sys.path \n",
    "clean_data_path = \"C:\\\\Repository\\\\DS-Intership-data\\\\clean_data\\\\\"\n",
    "sys.path.append(clean_data_path)\n",
    "\n",
    "# initiate dict for data\n",
    "to_read_clean_data = {}\n",
    "\n",
    "# fill to_read\n",
    "for dir_name, _, files in os.walk(clean_data_path):\n",
    "    for file in files:\n",
    "        to_read_clean_data[file] = dir_name + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "192bc216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'items.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\clean_data\\\\items.csv',\n",
       " 'item_categories.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\clean_data\\\\item_categories.csv',\n",
       " 'sales_train.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\clean_data\\\\sales_train.csv',\n",
       " 'sample_submission.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\clean_data\\\\sample_submission.csv',\n",
       " 'shops.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\clean_data\\\\shops.csv',\n",
       " 'test.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\clean_data\\\\test.csv'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to_read\n",
    "to_read_clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "574757dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.22 s\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = {}\n",
    "# read data\n",
    "for file, path in to_read_clean_data.items():\n",
    "    data[file.split('.')[0]] = pd.read_csv(os.path.join(os.path.dirname(path), file), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0244d",
   "metadata": {},
   "source": [
    "Cluster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b533687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data path to sys.path \n",
    "cluster_data_path = \"C:\\\\Repository\\\\DS-Intership-data\\\\cluster_data\\\\\"\n",
    "sys.path.append(cluster_data_path)\n",
    "\n",
    "# initiate dict for data\n",
    "to_read_cluster_data = {}\n",
    "\n",
    "# fill to_read\n",
    "for dir_name, _, files in os.walk(cluster_data_path):\n",
    "    for file in files:\n",
    "        to_read_cluster_data[file] = dir_name + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f036886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_category_clusters.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\cluster_data\\\\item_category_clusters.csv',\n",
       " 'item_price_clusters.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\cluster_data\\\\item_price_clusters.csv',\n",
       " 'shop_clusters.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\cluster_data\\\\shop_clusters.csv',\n",
       " 'subtype_clusters.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\cluster_data\\\\subtype_clusters.csv',\n",
       " 'type_code_clusters.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\cluster_data\\\\type_code_clusters.csv'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to_read\n",
    "to_read_cluster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32ecf8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 328 ms\n",
      "Wall time: 398 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cluster_data = {}\n",
    "# read data\n",
    "for file, path in to_read_cluster_data.items():\n",
    "    cluster_data[file.split('.')[0]] = pd.read_csv(os.path.join(os.path.dirname(path), file), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7469ba2",
   "metadata": {},
   "source": [
    "### Data preproccesing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0104ac",
   "metadata": {},
   "source": [
    "###### Reduce memory usage for big dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c883c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0aaf12",
   "metadata": {},
   "source": [
    "###### Build pretrained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6af7412a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.73 s\n",
      "Wall time: 5.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# combinate month, shop and item in order of increasing month\n",
    "pretrained_data  = []\n",
    "cols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\n",
    "for i in data['sales_train'].date_block_num.unique():\n",
    "    sales = data['sales_train'][data['sales_train'].date_block_num == i]\n",
    "    pretrained_data.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique()))))\n",
    "\n",
    "pretrained_data = pd.DataFrame(np.vstack(pretrained_data), columns = cols)\n",
    "pretrained_data.sort_values(cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "995cfc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 344 ms\n",
      "Wall time: 355 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10902924, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# add profit from sales\n",
    "pretrained_data[\"profit\"] = data['sales_train'][\"item_cnt_day\"] * data['sales_train'][\"item_price\"]\n",
    "pretrained_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18cec5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.3 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pretrained_data = dd.from_pandas(pretrained_data, npartitions=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "242fb13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 27.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Delayed('int-05bfa734-6bb0-46de-b10f-7e63ddd7e4da'), 9)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# merge datasets for data preprocessing\n",
    "pretrained_data = dd.merge(pretrained_data, data['shops'], on = [\"shop_id\"], how = \"left\" )\n",
    "pretrained_data = dd.merge(pretrained_data, data['items'], on = [\"item_id\"], how = \"left\")\n",
    "pretrained_data = dd.merge(pretrained_data, data['item_categories'], on = [\"item_category_id\"], how = \"left\" )\n",
    "pretrained_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "544333b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "CPU times: total: 438 ms\n",
      "Wall time: 465 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Delayed('int-dda4a48b-8b03-4fd8-8f4a-54e725ac2de0'), 10)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# add count of sold items per month\n",
    "group = data['sales_train'].groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg({\"item_cnt_day\": [\"sum\"]})\n",
    "group.columns = [\"item_cnt_month\"]\n",
    "group.reset_index(inplace = True)\n",
    "group = dd.from_pandas(group, npartitions=8)\n",
    "pretrained_data = dd.merge(pretrained_data, group, on = cols, how = \"left\")\n",
    "pretrained_data[\"item_cnt_month\"] = pretrained_data[\"item_cnt_month\"].fillna(0)\n",
    "pretrained_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f8269",
   "metadata": {},
   "source": [
    "###### Substruct some categories from string data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e416406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring exception in ensure_cleanup_on_exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 930, in ensure_cleanup_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 945, in shuffle_group_3\n",
      "    p.append(d, fsync=True)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 23, in append\n",
      "    data = valmap(self.encode, data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py\", line 85, in valmap\n",
      "    rv.update(zip(d.keys(), map(func, d.values())))\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 180, in serialize\n",
      "    col_header, col_bytes = index_to_header_bytes(df.columns)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 113, in index_to_header_bytes\n",
      "    header = (type(ind), ind._get_attributes_dict(), values.dtype, cat)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Index' object has no attribute '_get_attributes_dict'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 935, in ensure_cleanup_on_exception\n",
      "    p.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 39, in drop\n",
      "    return self.partd.drop()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\buffer.py\", line 78, in drop\n",
      "    self.slow.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\file.py\", line 97, in drop\n",
      "    os.mkdir(self.path)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\maxim\\\\AppData\\\\Local\\\\Temp\\\\tmpbsma7m3i.partd'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_get_attributes_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:3\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114\u001b[0m, in \u001b[0;36mLabelEncoder.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m        Encoded labels.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_, y \u001b[38;5;241m=\u001b[39m _unique(y, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1220\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ravel column or 1d numpy array, else raises an error.\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \n\u001b[0;32m   1196\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;124;03m    If `y` is not a 1D array or a 2D array with a single row or column.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y)\n\u001b[1;32m-> 1220\u001b[0m y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1221\u001b[0m     y,\n\u001b[0;32m   1222\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1223\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1224\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1225\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1226\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   1227\u001b[0m )\n\u001b[0;32m   1229\u001b[0m shape \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:598\u001b[0m, in \u001b[0;36m_Frame.__array__\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m    599\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed)\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:310\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:595\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[0;32m    593\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m--> 595\u001b[0m results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[38;5;241m.\u001b[39m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[38;5;241m=\u001b[39m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[38;5;241m=\u001b[39mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[0;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[0;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m _execute_task(task, data)\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[0;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\core.py:121\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    117\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py:945\u001b[0m, in \u001b[0;36mshuffle_group_3\u001b[1;34m(df, col, npartitions, p)\u001b[0m\n\u001b[0;32m    943\u001b[0m g \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(col)\n\u001b[0;32m    944\u001b[0m d \u001b[38;5;241m=\u001b[39m {i: g\u001b[38;5;241m.\u001b[39mget_group(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m g\u001b[38;5;241m.\u001b[39mgroups}\n\u001b[1;32m--> 945\u001b[0m p\u001b[38;5;241m.\u001b[39mappend(d, fsync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\partd\\encode.py:23\u001b[0m, in \u001b[0;36mEncode.append\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 23\u001b[0m     data \u001b[38;5;241m=\u001b[39m valmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode, data)\n\u001b[0;32m     24\u001b[0m     data \u001b[38;5;241m=\u001b[39m valmap(frame, data)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartd\u001b[38;5;241m.\u001b[39mappend(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py:85\u001b[0m, in \u001b[0;36mvalmap\u001b[1;34m(func, d, factory)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Apply function to values of dictionary\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m>>> bills = {\"Alice\": [20, 15, 30], \"Bob\": [10, 35]}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    itemmap\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m rv \u001b[38;5;241m=\u001b[39m factory()\n\u001b[1;32m---> 85\u001b[0m rv\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mzip\u001b[39m(d\u001b[38;5;241m.\u001b[39mkeys(), \u001b[38;5;28mmap\u001b[39m(func, d\u001b[38;5;241m.\u001b[39mvalues())))\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rv\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\partd\\pandas.py:180\u001b[0m, in \u001b[0;36mserialize\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserialize\u001b[39m(df):\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Serialize and compress a Pandas DataFrame\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    Uses Pandas blocks, snappy, and blosc to deconstruct an array into bytes\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     col_header, col_bytes \u001b[38;5;241m=\u001b[39m index_to_header_bytes(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    181\u001b[0m     ind_header, ind_bytes \u001b[38;5;241m=\u001b[39m index_to_header_bytes(df\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m    182\u001b[0m     headers \u001b[38;5;241m=\u001b[39m [col_header, ind_header]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\partd\\pandas.py:113\u001b[0m, in \u001b[0;36mindex_to_header_bytes\u001b[1;34m(ind)\u001b[0m\n\u001b[0;32m    110\u001b[0m     cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     values \u001b[38;5;241m=\u001b[39m ind\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 113\u001b[0m header \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mtype\u001b[39m(ind), ind\u001b[38;5;241m.\u001b[39m_get_attributes_dict(), values\u001b[38;5;241m.\u001b[39mdtype, cat)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m pnp\u001b[38;5;241m.\u001b[39mcompress(pnp\u001b[38;5;241m.\u001b[39mserialize(values), values\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m header, \u001b[38;5;28mbytes\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Index' object has no attribute '_get_attributes_dict'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# item categories data\n",
    "pretrained_data[\"type_code\"] = pretrained_data.item_category_name.apply(lambda x: x.split(\" \")[0]).astype(str)\n",
    "pretrained_data.type_code = LabelEncoder().fit_transform(pretrained_data.type_code)\n",
    "pretrained_data[\"split\"] = pretrained_data.item_category_name.apply(lambda x: x.split(\"-\"))\n",
    "pretrained_data[\"subtype\"] = pretrained_data.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "pretrained_data[\"subtype_code\"] = LabelEncoder().fit_transform(pretrained_data[\"subtype\"])\n",
    "pretrained_data = pretrained_data.drop(columns=['item_category_name', 'split', 'subtype'])\n",
    "pretrained_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b765920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring exception in ensure_cleanup_on_exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 930, in ensure_cleanup_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 945, in shuffle_group_3\n",
      "    p.append(d, fsync=True)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 23, in append\n",
      "    data = valmap(self.encode, data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py\", line 85, in valmap\n",
      "    rv.update(zip(d.keys(), map(func, d.values())))\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 180, in serialize\n",
      "    col_header, col_bytes = index_to_header_bytes(df.columns)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 113, in index_to_header_bytes\n",
      "    header = (type(ind), ind._get_attributes_dict(), values.dtype, cat)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Index' object has no attribute '_get_attributes_dict'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 935, in ensure_cleanup_on_exception\n",
      "    p.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 39, in drop\n",
      "    return self.partd.drop()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\buffer.py\", line 78, in drop\n",
      "    self.slow.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\file.py\", line 95, in drop\n",
      "    shutil.rmtree(self.path)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 759, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 603, in _rmtree_unsafe\n",
      "    onerror(os.scandir, path, sys.exc_info())\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 600, in _rmtree_unsafe\n",
      "    with os.scandir(path) as scandir_it:\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\maxim\\\\AppData\\\\Local\\\\Temp\\\\tmpzk1w1rc4.partd'\n",
      "ignoring exception in ensure_cleanup_on_exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 930, in ensure_cleanup_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 945, in shuffle_group_3\n",
      "    p.append(d, fsync=True)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 23, in append\n",
      "    data = valmap(self.encode, data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py\", line 85, in valmap\n",
      "    rv.update(zip(d.keys(), map(func, d.values())))\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 180, in serialize\n",
      "    col_header, col_bytes = index_to_header_bytes(df.columns)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 113, in index_to_header_bytes\n",
      "    header = (type(ind), ind._get_attributes_dict(), values.dtype, cat)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Index' object has no attribute '_get_attributes_dict'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 935, in ensure_cleanup_on_exception\n",
      "    p.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 39, in drop\n",
      "    return self.partd.drop()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\buffer.py\", line 78, in drop\n",
      "    self.slow.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\file.py\", line 97, in drop\n",
      "    os.mkdir(self.path)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\maxim\\\\AppData\\\\Local\\\\Temp\\\\tmpzk1w1rc4.partd'\n",
      "ignoring exception in ensure_cleanup_on_exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 930, in ensure_cleanup_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 945, in shuffle_group_3\n",
      "    p.append(d, fsync=True)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 23, in append\n",
      "    data = valmap(self.encode, data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py\", line 85, in valmap\n",
      "    rv.update(zip(d.keys(), map(func, d.values())))\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 180, in serialize\n",
      "    col_header, col_bytes = index_to_header_bytes(df.columns)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 113, in index_to_header_bytes\n",
      "    header = (type(ind), ind._get_attributes_dict(), values.dtype, cat)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Index' object has no attribute '_get_attributes_dict'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 935, in ensure_cleanup_on_exception\n",
      "    p.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 39, in drop\n",
      "    return self.partd.drop()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\buffer.py\", line 78, in drop\n",
      "    self.slow.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\file.py\", line 95, in drop\n",
      "    shutil.rmtree(self.path)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 759, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 626, in _rmtree_unsafe\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 624, in _rmtree_unsafe\n",
      "    os.rmdir(path)\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\maxim\\\\AppData\\\\Local\\\\Temp\\\\tmpzk1w1rc4.partd'\n",
      "ignoring exception in ensure_cleanup_on_exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 930, in ensure_cleanup_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 945, in shuffle_group_3\n",
      "    p.append(d, fsync=True)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 23, in append\n",
      "    data = valmap(self.encode, data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py\", line 85, in valmap\n",
      "    rv.update(zip(d.keys(), map(func, d.values())))\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 180, in serialize\n",
      "    col_header, col_bytes = index_to_header_bytes(df.columns)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 113, in index_to_header_bytes\n",
      "    header = (type(ind), ind._get_attributes_dict(), values.dtype, cat)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Index' object has no attribute '_get_attributes_dict'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 935, in ensure_cleanup_on_exception\n",
      "    p.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 39, in drop\n",
      "    return self.partd.drop()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\buffer.py\", line 78, in drop\n",
      "    self.slow.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\file.py\", line 97, in drop\n",
      "    os.mkdir(self.path)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\maxim\\\\AppData\\\\Local\\\\Temp\\\\tmpzk1w1rc4.partd'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_get_attributes_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114\u001b[0m, in \u001b[0;36mLabelEncoder.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m        Encoded labels.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_, y \u001b[38;5;241m=\u001b[39m _unique(y, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1220\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ravel column or 1d numpy array, else raises an error.\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \n\u001b[0;32m   1196\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;124;03m    If `y` is not a 1D array or a 2D array with a single row or column.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y)\n\u001b[1;32m-> 1220\u001b[0m y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1221\u001b[0m     y,\n\u001b[0;32m   1222\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1223\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1224\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1225\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1226\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   1227\u001b[0m )\n\u001b[0;32m   1229\u001b[0m shape \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:598\u001b[0m, in \u001b[0;36m_Frame.__array__\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m    599\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed)\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:310\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:595\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[0;32m    593\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m--> 595\u001b[0m results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[38;5;241m.\u001b[39m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[38;5;241m=\u001b[39m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[38;5;241m=\u001b[39mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[0;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[0;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m _execute_task(task, data)\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[0;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\core.py:121\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    117\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py:945\u001b[0m, in \u001b[0;36mshuffle_group_3\u001b[1;34m(df, col, npartitions, p)\u001b[0m\n\u001b[0;32m    943\u001b[0m g \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(col)\n\u001b[0;32m    944\u001b[0m d \u001b[38;5;241m=\u001b[39m {i: g\u001b[38;5;241m.\u001b[39mget_group(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m g\u001b[38;5;241m.\u001b[39mgroups}\n\u001b[1;32m--> 945\u001b[0m p\u001b[38;5;241m.\u001b[39mappend(d, fsync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\partd\\encode.py:23\u001b[0m, in \u001b[0;36mEncode.append\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 23\u001b[0m     data \u001b[38;5;241m=\u001b[39m valmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode, data)\n\u001b[0;32m     24\u001b[0m     data \u001b[38;5;241m=\u001b[39m valmap(frame, data)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartd\u001b[38;5;241m.\u001b[39mappend(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py:85\u001b[0m, in \u001b[0;36mvalmap\u001b[1;34m(func, d, factory)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Apply function to values of dictionary\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m>>> bills = {\"Alice\": [20, 15, 30], \"Bob\": [10, 35]}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    itemmap\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m rv \u001b[38;5;241m=\u001b[39m factory()\n\u001b[1;32m---> 85\u001b[0m rv\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mzip\u001b[39m(d\u001b[38;5;241m.\u001b[39mkeys(), \u001b[38;5;28mmap\u001b[39m(func, d\u001b[38;5;241m.\u001b[39mvalues())))\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rv\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\partd\\pandas.py:180\u001b[0m, in \u001b[0;36mserialize\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserialize\u001b[39m(df):\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Serialize and compress a Pandas DataFrame\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    Uses Pandas blocks, snappy, and blosc to deconstruct an array into bytes\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     col_header, col_bytes \u001b[38;5;241m=\u001b[39m index_to_header_bytes(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    181\u001b[0m     ind_header, ind_bytes \u001b[38;5;241m=\u001b[39m index_to_header_bytes(df\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m    182\u001b[0m     headers \u001b[38;5;241m=\u001b[39m [col_header, ind_header]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\partd\\pandas.py:113\u001b[0m, in \u001b[0;36mindex_to_header_bytes\u001b[1;34m(ind)\u001b[0m\n\u001b[0;32m    110\u001b[0m     cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     values \u001b[38;5;241m=\u001b[39m ind\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 113\u001b[0m header \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mtype\u001b[39m(ind), ind\u001b[38;5;241m.\u001b[39m_get_attributes_dict(), values\u001b[38;5;241m.\u001b[39mdtype, cat)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m pnp\u001b[38;5;241m.\u001b[39mcompress(pnp\u001b[38;5;241m.\u001b[39mserialize(values), values\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m header, \u001b[38;5;28mbytes\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Index' object has no attribute '_get_attributes_dict'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# shops data\n",
    "pretrained_data[\"shop_city\"] = pretrained_data.shop_name.str.split(\" \").map(lambda x: x[0])\n",
    "pretrained_data[\"shop_category\"] = pretrained_data.shop_name.str.split(\" \").map(lambda x: x[1])\n",
    "pretrained_data[\"shop_category\"] = LabelEncoder().fit_transform(pretrained_data.shop_category)\n",
    "pretrained_data[\"shop_city\"] = LabelEncoder().fit_transform(pretrained_data.shop_city)\n",
    "pretrained_data = pretrained_data.drop(columns=['shop_name', 'city'])\n",
    "pretrained_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45e0db08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring exception in ensure_cleanup_on_exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 930, in ensure_cleanup_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 945, in shuffle_group_3\n",
      "    p.append(d, fsync=True)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 23, in append\n",
      "    data = valmap(self.encode, data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py\", line 85, in valmap\n",
      "    rv.update(zip(d.keys(), map(func, d.values())))\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 180, in serialize\n",
      "    col_header, col_bytes = index_to_header_bytes(df.columns)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 113, in index_to_header_bytes\n",
      "    header = (type(ind), ind._get_attributes_dict(), values.dtype, cat)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Index' object has no attribute '_get_attributes_dict'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 935, in ensure_cleanup_on_exception\n",
      "    p.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 39, in drop\n",
      "    return self.partd.drop()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\buffer.py\", line 78, in drop\n",
      "    self.slow.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\file.py\", line 97, in drop\n",
      "    os.mkdir(self.path)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\maxim\\\\AppData\\\\Local\\\\Temp\\\\tmpljx3oiav.partd'\n",
      "ignoring exception in ensure_cleanup_on_exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 930, in ensure_cleanup_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 945, in shuffle_group_3\n",
      "    p.append(d, fsync=True)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 23, in append\n",
      "    data = valmap(self.encode, data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py\", line 85, in valmap\n",
      "    rv.update(zip(d.keys(), map(func, d.values())))\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 180, in serialize\n",
      "    col_header, col_bytes = index_to_header_bytes(df.columns)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 113, in index_to_header_bytes\n",
      "    header = (type(ind), ind._get_attributes_dict(), values.dtype, cat)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Index' object has no attribute '_get_attributes_dict'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 935, in ensure_cleanup_on_exception\n",
      "    p.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 39, in drop\n",
      "    return self.partd.drop()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\buffer.py\", line 78, in drop\n",
      "    self.slow.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\file.py\", line 95, in drop\n",
      "    shutil.rmtree(self.path)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 759, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 626, in _rmtree_unsafe\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 624, in _rmtree_unsafe\n",
      "    os.rmdir(path)\n",
      "PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\maxim\\\\AppData\\\\Local\\\\Temp\\\\tmpljx3oiav.partd'\n",
      "ignoring exception in ensure_cleanup_on_exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 930, in ensure_cleanup_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 945, in shuffle_group_3\n",
      "    p.append(d, fsync=True)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 23, in append\n",
      "    data = valmap(self.encode, data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py\", line 85, in valmap\n",
      "    rv.update(zip(d.keys(), map(func, d.values())))\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 180, in serialize\n",
      "    col_header, col_bytes = index_to_header_bytes(df.columns)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 113, in index_to_header_bytes\n",
      "    header = (type(ind), ind._get_attributes_dict(), values.dtype, cat)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Index' object has no attribute '_get_attributes_dict'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 935, in ensure_cleanup_on_exception\n",
      "    p.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 39, in drop\n",
      "    return self.partd.drop()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\buffer.py\", line 78, in drop\n",
      "    self.slow.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\file.py\", line 97, in drop\n",
      "    os.mkdir(self.path)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\maxim\\\\AppData\\\\Local\\\\Temp\\\\tmpljx3oiav.partd'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_get_attributes_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:11\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114\u001b[0m, in \u001b[0;36mLabelEncoder.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m        Encoded labels.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_, y \u001b[38;5;241m=\u001b[39m _unique(y, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1220\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ravel column or 1d numpy array, else raises an error.\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \n\u001b[0;32m   1196\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;124;03m    If `y` is not a 1D array or a 2D array with a single row or column.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y)\n\u001b[1;32m-> 1220\u001b[0m y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1221\u001b[0m     y,\n\u001b[0;32m   1222\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1223\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1224\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1225\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1226\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   1227\u001b[0m )\n\u001b[0;32m   1229\u001b[0m shape \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:598\u001b[0m, in \u001b[0;36m_Frame.__array__\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m    599\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed)\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:310\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:595\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[0;32m    593\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m--> 595\u001b[0m results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[38;5;241m.\u001b[39m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[38;5;241m=\u001b[39m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[38;5;241m=\u001b[39mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[0;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[0;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m _execute_task(task, data)\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[0;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\core.py:121\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    117\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py:945\u001b[0m, in \u001b[0;36mshuffle_group_3\u001b[1;34m(df, col, npartitions, p)\u001b[0m\n\u001b[0;32m    943\u001b[0m g \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(col)\n\u001b[0;32m    944\u001b[0m d \u001b[38;5;241m=\u001b[39m {i: g\u001b[38;5;241m.\u001b[39mget_group(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m g\u001b[38;5;241m.\u001b[39mgroups}\n\u001b[1;32m--> 945\u001b[0m p\u001b[38;5;241m.\u001b[39mappend(d, fsync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\partd\\encode.py:23\u001b[0m, in \u001b[0;36mEncode.append\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 23\u001b[0m     data \u001b[38;5;241m=\u001b[39m valmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode, data)\n\u001b[0;32m     24\u001b[0m     data \u001b[38;5;241m=\u001b[39m valmap(frame, data)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartd\u001b[38;5;241m.\u001b[39mappend(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py:85\u001b[0m, in \u001b[0;36mvalmap\u001b[1;34m(func, d, factory)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Apply function to values of dictionary\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m>>> bills = {\"Alice\": [20, 15, 30], \"Bob\": [10, 35]}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    itemmap\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m rv \u001b[38;5;241m=\u001b[39m factory()\n\u001b[1;32m---> 85\u001b[0m rv\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mzip\u001b[39m(d\u001b[38;5;241m.\u001b[39mkeys(), \u001b[38;5;28mmap\u001b[39m(func, d\u001b[38;5;241m.\u001b[39mvalues())))\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rv\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\partd\\pandas.py:180\u001b[0m, in \u001b[0;36mserialize\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserialize\u001b[39m(df):\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Serialize and compress a Pandas DataFrame\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    Uses Pandas blocks, snappy, and blosc to deconstruct an array into bytes\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     col_header, col_bytes \u001b[38;5;241m=\u001b[39m index_to_header_bytes(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    181\u001b[0m     ind_header, ind_bytes \u001b[38;5;241m=\u001b[39m index_to_header_bytes(df\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m    182\u001b[0m     headers \u001b[38;5;241m=\u001b[39m [col_header, ind_header]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\partd\\pandas.py:113\u001b[0m, in \u001b[0;36mindex_to_header_bytes\u001b[1;34m(ind)\u001b[0m\n\u001b[0;32m    110\u001b[0m     cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     values \u001b[38;5;241m=\u001b[39m ind\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 113\u001b[0m header \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mtype\u001b[39m(ind), ind\u001b[38;5;241m.\u001b[39m_get_attributes_dict(), values\u001b[38;5;241m.\u001b[39mdtype, cat)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m pnp\u001b[38;5;241m.\u001b[39mcompress(pnp\u001b[38;5;241m.\u001b[39mserialize(values), values\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m header, \u001b[38;5;28mbytes\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Index' object has no attribute '_get_attributes_dict'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring exception in ensure_cleanup_on_exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 930, in ensure_cleanup_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 945, in shuffle_group_3\n",
      "    p.append(d, fsync=True)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 23, in append\n",
      "    data = valmap(self.encode, data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\toolz\\dicttoolz.py\", line 85, in valmap\n",
      "    rv.update(zip(d.keys(), map(func, d.values())))\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 180, in serialize\n",
      "    col_header, col_bytes = index_to_header_bytes(df.columns)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\pandas.py\", line 113, in index_to_header_bytes\n",
      "    header = (type(ind), ind._get_attributes_dict(), values.dtype, cat)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Index' object has no attribute '_get_attributes_dict'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 935, in ensure_cleanup_on_exception\n",
      "    p.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\encode.py\", line 39, in drop\n",
      "    return self.partd.drop()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\buffer.py\", line 78, in drop\n",
      "    self.slow.drop()\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\site-packages\\partd\\file.py\", line 95, in drop\n",
      "    shutil.rmtree(self.path)\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 759, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 626, in _rmtree_unsafe\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"C:\\Users\\maxim\\anaconda3\\Lib\\shutil.py\", line 624, in _rmtree_unsafe\n",
      "    os.rmdir(path)\n",
      "PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\maxim\\\\AppData\\\\Local\\\\Temp\\\\tmpljx3oiav.partd'\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# item data\n",
    "pretrained_data[\"name1\"], pretrained_data[\"name2\"] = pretrained_data.item_name.str.split('[', n=1).str[0], pretrained_data.item_name.str.split('[', n=1).str[1]\n",
    "pretrained_data[\"name1\"], pretrained_data[\"name3\"] = pretrained_data.item_name.str.split('(', n=1).str[0], pretrained_data.item_name.str.split('(', n=1).str[1]\n",
    "\n",
    "# replace special characters and turn to lower case\n",
    "pretrained_data[\"name2\"] = pretrained_data.name2.str.replace('[^A-Za-z0-9--]+', \" \").str.lower()\n",
    "pretrained_data[\"name3\"] = pretrained_data.name3.str.replace('[^A-Za-z0-9--]+', \" \").str.lower()\n",
    "\n",
    "pretrained_data = pretrained_data.fillna('0')\n",
    "\n",
    "pretrained_data.name2 = LabelEncoder().fit_transform(pretrained_data.name2)\n",
    "pretrained_data.name3 = LabelEncoder().fit_transform(pretrained_data.name3)\n",
    "\n",
    "pretrained_data = pretrained_data.drop(columns=['name1', 'item_name']) \n",
    "pretrained_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d2309",
   "metadata": {},
   "source": [
    "###### time series feature extration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "52f3911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_lag_feature(df, lags, col):\n",
    "#     tmp = df[['date_block_num', 'shop_id', 'item_id', col]]\n",
    "#     for i in lags:\n",
    "#         shifted = tmp.copy()\n",
    "#         shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "#         shifted['date_block_num'] += i\n",
    "#         df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "52e963a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # item count per month lag\n",
    "# pretrained_data = extract_lag_feature(pretrained_data, [1], 'item_cnt_month')\n",
    "# pretrained_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f474cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # lag of average item count sales per month \n",
    "\n",
    "# group = pretrained_data.groupby([\"date_block_num\"] ).agg({\"item_cnt_month\" : \"mean\"})\n",
    "# group.columns = [\"avg_by_month_item_cnt\"]\n",
    "# group.reset_index()\n",
    "\n",
    "\n",
    "# pretrained_data = pd.merge(pretrained_data, group, on = [\"date_block_num\"], how = \"left\")\n",
    "# del(group)\n",
    "# pretrained_data = extract_lag_feature(pretrained_data, [1], \"avg_by_month_item_cnt\")\n",
    "# pretrained_data.drop([\"avg_by_month_item_cnt\"], axis = 1, inplace = True)\n",
    "# pretrained_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f484f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # lag of average item sales per month of each item_id\n",
    "\n",
    "# group = pretrained_data.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': 'mean'})\n",
    "# group.columns = ['avg_by_month_item_id_item_cnt']\n",
    "# group = group.reset_index()\n",
    "\n",
    "\n",
    "# pretrained_data = pd.merge(pretrained_data, group, on=['date_block_num', 'item_id'], how='left')\n",
    "# del(group)\n",
    "# pretrained_data = extract_lag_feature(pretrained_data, [1], \"avg_by_month_item_id_item_cnt\")\n",
    "# pretrained_data.drop(columns = ['avg_by_month_item_id_item_cnt'], axis = 1, inplace = True)\n",
    "# pretrained_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d8087d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # lag of average item sales per month of each shop\n",
    "\n",
    "# group = pretrained_data.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': 'mean'})\n",
    "# group.columns = ['avg_by_month_shop_item_cnt']\n",
    "# group = group.reset_index()\n",
    "\n",
    "# pretrained_data = pd.merge(pretrained_data, group, on=['date_block_num', 'shop_id'], how='left')\n",
    "\n",
    "# pretrained_data = extract_lag_feature(pretrained_data, [1], \"avg_by_month_shop_item_cnt\")\n",
    "# pretrained_data.drop(columns = ['avg_by_month_shop_item_cnt'], axis = 1, inplace = True)\n",
    "# pretrained_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "536b5abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # lag of average item sales per month of each city\n",
    "\n",
    "# group = pretrained_data.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': 'mean'})\n",
    "# group.columns = ['avg_by_month_city_item_cnt']\n",
    "# group = group.reset_index()\n",
    "\n",
    "\n",
    "# pretrained_data = pd.merge(pretrained_data, group, on=['date_block_num', 'shop_city'], how='left')\n",
    "# del(group)\n",
    "# pretrained_data = extract_lag_feature( pretrained_data, [1], \"avg_by_month_city_item_cnt\" )\n",
    "# pretrained_data.drop(columns = ['avg_by_month_city_item_cnt'], axis = 1, inplace = True)\n",
    "# pretrained_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e974248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # lag of average item sales per month of each category\n",
    "\n",
    "# group = pretrained_data.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': 'mean'})\n",
    "# group.columns = ['avg_by_month_cat_item_cnt']\n",
    "# group = group.reset_index()\n",
    "\n",
    "\n",
    "# pretrained_data = pd.merge(pretrained_data, group, on=['date_block_num', 'item_category_id'], how='left')\n",
    "# del(group)\n",
    "# pretrained_data = extract_lag_feature(pretrained_data, [1], \"avg_by_month_cat_item_cnt\")\n",
    "# pretrained_data.drop(columns = ['avg_by_month_cat_item_cnt'], axis = 1, inplace = True)\n",
    "# pretrained_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4230003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # price mean grouped by item_id\n",
    "\n",
    "# group = data['sales_train'].groupby(['item_id']).agg({'item_price': ['mean']})\n",
    "# group.columns = ['avg_item_price']\n",
    "# group.reset_index(inplace=True)\n",
    "\n",
    "# pretrained_data = pd.merge(pretrained_data, group, on=['item_id'], how='left')\n",
    "# del(group)\n",
    "\n",
    "# # add price mean grouped by month and item_id\n",
    "# group = data['sales_train'].groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\n",
    "# group.columns = ['avg_item_price_month']\n",
    "# group.reset_index(inplace=True)\n",
    "\n",
    "# pretrained_data = pd.merge(pretrained_data, group, on=['date_block_num','item_id'], how='left')\n",
    "# del(group)\n",
    "# pretrained_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0d8b41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_trends(row) :\n",
    "#     for i in lags:\n",
    "#         if row[\"delta_price_lag_\" + str(i)]:\n",
    "#             return row[\"delta_price_lag_\" + str(i)]\n",
    "#     return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e6ea0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate lag of month column to provide price for test set\n",
    "# lags = [1]\n",
    "# pretrained_data = extract_lag_feature(pretrained_data, lags, \"avg_item_price_month\")\n",
    "# pretrained_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f9557f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculation of lag of month column to provide price for test set (change of sales of item/price by month)\n",
    "# for i in lags:\n",
    "#     pretrained_data[\"delta_price_lag_\" + str(i) ] = (pretrained_data[\"avg_item_price_month_lag_\" + str(i)]\\\n",
    "#                                                      - pretrained_data[\"avg_item_price\"] )\\\n",
    "#                                                      / pretrained_data[\"avg_item_price\"]\n",
    "\n",
    "# pretrained_data[\"delta_price_lag\"] = pretrained_data.apply(select_trends, axis = 1)\n",
    "# pretrained_data[\"delta_price_lag\"].fillna(0, inplace = True)\n",
    "\n",
    "# features_to_drop = [\"avg_item_price_month\", \"avg_item_price\"]\n",
    "# for i in lags:\n",
    "#     features_to_drop.append(\"avg_item_price_month_lag_\" + str(i))\n",
    "#     features_to_drop.append(\"delta_price_lag_\" + str(i))\n",
    "# pretrained_data.drop(features_to_drop, axis = 1, inplace = True)\n",
    "# pretrained_data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ddcb7",
   "metadata": {},
   "source": [
    "###### Add clusters features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "030c87e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 656 ms\n",
      "Wall time: 692 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10902924, 13)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# categories clusterization\n",
    "pretrained_data = pd.merge(pretrained_data, cluster_data['item_category_clusters'], on='item_category_id', how='left')\n",
    "pretrained_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d127a69",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 21.3 GiB for an array with shape (2857479640,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:162\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    148\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    149\u001b[0m         left,\n\u001b[0;32m    150\u001b[0m         right,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:809\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m--> 809\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[0;32m    811\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    812\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    813\u001b[0m )\n\u001b[0;32m    814\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1065\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1062\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1063\u001b[0m     )\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1065\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_indexers()\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1038\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_join_indexers\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp]]:\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"return the join indexers\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_join_indexers(\n\u001b[0;32m   1039\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow\n\u001b[0;32m   1040\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1690\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[0;32m   1680\u001b[0m join_func \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39minner_join,\n\u001b[0;32m   1682\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39mleft_outer_join,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39mfull_outer_join,\n\u001b[0;32m   1687\u001b[0m }[how]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;66;03m# error: Cannot call function of unknown type\u001b[39;00m\n\u001b[1;32m-> 1690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m join_func(lkey, rkey, count, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\join.pyx:142\u001b[0m, in \u001b[0;36mpandas._libs.join.left_outer_join\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\algos.pyx:238\u001b[0m, in \u001b[0;36mpandas._libs.algos.groupsort_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 21.3 GiB for an array with shape (2857479640,) and data type int64"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# price clusterization\n",
    "pretrained_data = pd.merge(pretrained_data, cluster_data['item_price_clusters'], on='item_id', how='left')\n",
    "pretrained_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e022ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# shop clusterization\n",
    "pretrained_data = pd.merge(pretrained_data, cluster_data['shop_clusters'], on='shop_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef3606",
   "metadata": {},
   "source": [
    "###### Other usefull features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dates of fisrt sale for each shop and for each item\n",
    "# pretrained_data['item_shop_first_sale'] = pretrained_data['date_block_num'] - pretrained_data.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n",
    "# pretrained_data['item_first_sale'] = pretrained_data['date_block_num'] - pretrained_data.groupby('item_id')['date_block_num'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c47ec3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>profit</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>type_code</th>\n",
       "      <th>subtype_code</th>\n",
       "      <th>shop_city</th>\n",
       "      <th>shop_category</th>\n",
       "      <th>name2</th>\n",
       "      <th>name3</th>\n",
       "      <th>category_cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10902922</th>\n",
       "      <td>33</td>\n",
       "      <td>59</td>\n",
       "      <td>22166</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>176</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10902923</th>\n",
       "      <td>33</td>\n",
       "      <td>59</td>\n",
       "      <td>22167</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_block_num  shop_id  item_id  item_category_id profit  \\\n",
       "10902922              33       59    22166                54      0   \n",
       "10902923              33       59    22167                49      0   \n",
       "\n",
       "          item_cnt_month  type_code  subtype_code  shop_city  shop_category  \\\n",
       "10902922            0.00          8            61         30              8   \n",
       "10902923            0.00          8            39         30              8   \n",
       "\n",
       "          name2  name3  category_cluster_id  \n",
       "10902922    176     40                    4  \n",
       "10902923      5     27                    4  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "44f4a46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 249.55 Mb (22.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "# reduce memory usage\n",
    "pretrained_data = reduce_mem_usage(pretrained_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318530f",
   "metadata": {},
   "source": [
    "###### Load train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa65a5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 26.3 s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = {\n",
    "    'train_data' : pretrained_data,\n",
    "    'test_data' : data['test'],\n",
    "    'sample_submission' : data['sample_submission']\n",
    "}\n",
    "\n",
    "# add path to load to sys.path \n",
    "path_to_load = \"C:\\\\Repository\\\\DS-Intership-data\\\\train_test_data\\\\\"\n",
    "os.makedirs(path_to_load, exist_ok=True)\n",
    "sys.path.append(path_to_load)\n",
    "\n",
    "# load data \n",
    "for file, df in train_data.items():\n",
    "    df.to_csv(path_to_load+file+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae689a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv\n",
      " test_data.csv\n",
      " train_data.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "print(*[f+\"\\n\" for f in listdir(path_to_load) if isfile(join(path_to_load, f))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
