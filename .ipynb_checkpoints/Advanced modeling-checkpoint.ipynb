{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51743ebf",
   "metadata": {},
   "source": [
    "### Advanced modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11c72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra\n",
    "import numpy as np\n",
    "\n",
    "#working with data in table structers\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# working with files\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# to off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# validation schema \n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3a7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using ray engine for parallel calculation(for oprimization)\n",
    "# %env MODIN_ENGINE=ray\n",
    "# import modin.pandas as mpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc1db7",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764bc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data path to sys.path \n",
    "train_test_data_path = \"C:\\\\Repository\\\\DS-Intership-data\\\\train_test_data\\\\\"\n",
    "sys.path.append(train_test_data_path)\n",
    "\n",
    "# initiate dict for data\n",
    "to_read_train_test_data = {}\n",
    "\n",
    "# fill to_read\n",
    "for dir_name, _, files in os.walk(train_test_data_path):\n",
    "    for file in files:\n",
    "        to_read_train_test_data[file] = dir_name + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49be37be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_submission.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\train_test_data\\\\sample_submission.csv',\n",
       " 'test_data.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\train_test_data\\\\test_data.csv',\n",
       " 'train_data.csv': 'C:\\\\Repository\\\\DS-Intership-data\\\\train_test_data\\\\train_data.csv'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to_read\n",
    "to_read_train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fa37e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 41 s\n",
      "Wall time: 43.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = {}\n",
    "# read data\n",
    "for file, path in to_read_train_test_data.items():\n",
    "    data[file.split('.')[0]] = pd.read_csv(os.path.join(os.path.dirname(path), file), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca763ec5",
   "metadata": {},
   "source": [
    "### Machine learning pipeline modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1789bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, train_data,\n",
    "                test_data, metrics=['rmse'],\n",
    "                model=DecisionTreeRegressor(max_depth=1, random_state=42),\n",
    "                check_nans = True,\n",
    "                check_infs = True,\n",
    "                feature_importance_layer=True,\n",
    "                hyperparametr_optimization_layer=True,\n",
    "                explainability_layer=True,\n",
    "                error_analysis_layer=True\n",
    "                ):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.metrics = metrics\n",
    "        self.model = model\n",
    "        self.hyperparametr_optimization_layer=hyperparametr_optimization_layer\n",
    "        self.explainability_layer=explainability_layer\n",
    "        self.error_analysis_layer=error_analysis_layer\n",
    "\n",
    "        # Check data for valid columns\n",
    "        assert set([\n",
    "            'date_block_num',\n",
    "            'shop_id',\n",
    "            'item_category_id',\n",
    "            'item_id',\n",
    "            'item_cnt_month'\n",
    "        ]).issubset(train_data.columns),\\\n",
    "            \"Invalid data\"\n",
    "\n",
    "        assert set([\n",
    "            'shop_id',\n",
    "            'item_id',\n",
    "            'ID'\n",
    "        ]).issubset(test_data.columns),\\\n",
    "            \"Invalid data\"\n",
    "\n",
    "         # Check for valid variables\n",
    "        if check_nans:\n",
    "            assert train_data.isna().sum().sum() == 0, 'Data have NaNs'\n",
    "        if check_infs:\n",
    "            assert np.isfinite(train_data).sum().sum() != 0, 'Data have Infs'\n",
    "\n",
    "        self.X = train_data.drop(columns='item_cnt_month')\n",
    "        self.y = train_data[['item_id', 'shop_id', 'item_cnt_month']]\n",
    "        \n",
    "    def calculate_metrics(self, y_pred, y_true):\n",
    "        rmse = mse(y_true, y_pred, squared=True)\n",
    "        return rmse\n",
    "    \n",
    "    # Predict sales for target month (November 2015)\n",
    "    def predict_target(self, train_data=None):\n",
    "        assert train_data is None, \"train_data is not defined\"\n",
    "        \n",
    "        X_train, y_train = train_data[train_data.date_block_num != 34].drop(columns=['item_cnt_month']),\\\n",
    "                           train_data[train_data.date_block_num != 34].item_cnt_month\n",
    "        X_test, y_test = train_data[train_data.date_block_num == 34].drop(columns=['item_cnt_month']),\\\n",
    "                         train_data[train_data.date_block_num == 34].item_cnt_month\n",
    "        model = self.model\n",
    "        model.fit(X_train, y_train)\n",
    "        return X_test.join(pd.DataFrame(index=X_test.index, data=model.predict(X_test.values), columns=['item_cnt_month']))\\\n",
    "                                        [['item_id', 'shop_id', 'item_cnt_month']].\\\n",
    "                                        merge(self.test_data, on=['shop_id', 'item_id'], how='right').drop_duplicates()\n",
    "        \n",
    "    def feature_importance_layer(self, selector=\"Boruta\", sample_size=self.train_data):\n",
    "        if selector==\"Boruta\":\n",
    "            from boruta import BorutaPy\n",
    "            # select sample of data\n",
    "            X = self.train_data[self.train_data.date_block_num != 34].dropna().drop(columns='item_cnt_month')[:sample_size]\n",
    "            y = self.train_data[self.train_data.date_block_num != 34].dropna()['item_cnt_month'][:sample_size]\n",
    "            np.int = np.int_\n",
    "            np.float = np.float_\n",
    "            np.bool=np.bool_\n",
    "\n",
    "            # init selector\n",
    "            feat_selector = BorutaPy(RandomForestRegressor(max_depth=5, n_jobs=-1, n_estimators=20), \n",
    "                                     n_estimators=20, \n",
    "                                     verbose=0, \n",
    "                                     max_iter=20,\n",
    "                                     random_state=42,\n",
    "                                 )\n",
    "\n",
    "            # fit selector\n",
    "            feat_selector.fit(X.values, y.values)\n",
    "\n",
    "            # extract usefull features\n",
    "            mask = np.array(feat_selector.support_).reshape(4,8)\n",
    "            plt.imshow(mask);\n",
    "            plt.title(\"Feature selection\")\n",
    "            plt.show()\n",
    "            important_features = data['train_data'].drop(columns=['item_cnt_month']).iloc[:, feat_selector.support_]\n",
    "\n",
    "            # save info about usefull/useless features\n",
    "            feature_importnce_report = {\n",
    "                \"important_columns\": data['train_data'].drop(columns=['item_cnt_month'])\\\n",
    "                                                                       .iloc[:, feat_selector.support_].columns,\n",
    "                \"unimportant_columns\": data['train_data'].drop(columns=['item_cnt_month'])\\\n",
    "                                                                       .iloc[:, ~feat_selector.support_].columns\n",
    "            }\n",
    "            print(feature_importance_report)\n",
    "            \n",
    "            return important_features\n",
    "        \n",
    "    def hyperparametr_optimization_layer(self, optimazer=\"Grid\"):\n",
    "        if optimazer==\"Optuna\":\n",
    "            pass\n",
    "        if optimazer==\"Hyperopt\":\n",
    "            pass\n",
    "        \n",
    "    def explainability_layer(self):\n",
    "        pass\n",
    "    def error_analysis_layer(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self):\n",
    "        # optimal_hyperparametres = self.hyperparametr_optimization_layer()\n",
    "        if self.feature_importance_layer:\n",
    "            important_features = self.feature_importance_layer()\n",
    "        predictions = self.predict_target(important_features) # optimal_hyperparametres, important_features)\n",
    "        # model_explanation = self.explainability_layer()\n",
    "        # error_analysis = self.error_analysis_layer()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a0a0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.59 s\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pipeline = Pipeline(train_data=data['train_data'],\n",
    "                 test_data=data['test_data'],\n",
    "                 metrics=['rmse'],\n",
    "                 model = DecisionTreeRegressor(max_depth=2, random_state=42),\n",
    "                 check_nans=False,\n",
    "                 feature_importance_layer=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc6ef1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_pipeline.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
